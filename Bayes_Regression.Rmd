---
title: "Bayesian Regression models in ethnobotany"
author: "Cory Whitney"
github: "CWWhitney"
bibliography: 
  - bib/packages.bib
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(base)
library(bayesplot)
library(bayestestR)
library(broom)
library(ethnobotanyR)
library(insight)
library(knitr)
library(purrr)
library(Rcpp)
library(rmarkdown)
library(rstan)
library(rstanarm)
library(stats)
```

```{r, warning=FALSE, include = FALSE}
#Automatically write R library citation entries to a .bib file
knitr::write_bib(c(.packages(), 
                   'base',
                   'bayesplot',
                   'bayestestR', 
                   'broom',
                   'ethnobotanyR',
                   'insight',
                   'knitr',
                   'purrr',
                   'Rcpp',
                   'rmarkdown',
                   'rstanarm',
                   'rstan',
                   'stats'), 'bib/packages.bib')
```

# Linear and Bayesian regression

Here we compare two approaches to statistical inference in ethnobotany:

In *linear regression* data sampled from a population is considered to be random and the population parameter values are considered fixed (but unknown), known as *null hypothesis*. To estimate that *null hypothesis* we look for the sample parameters that maximize the likelihood of the data known as *p-value*. If we get very small p-value we tend to reject the null hypothesis. 

The *Bayesian regression*, in contrast, provides probabilities in an attempt to quantify the uncertainty about a certain hypothesis, but requires the use of a *prior* belief about how likely this hypothesis is to be true. The method then derives the probability of this hypothesis after seeing the data known as *posterior probability*. 

# Data preparation

For simplicity we simulate a data set with two numeric variables `proportion_used` and  `distance` and one categorical `Use_medicine`, with the target variable `abundance` of the plant species. This is similar to the data set we often use in the examples in the `ethnobotanyR` functions [@R-ethnobotanyR].

```{r}
#simulate a n_row ethnobotany data set with two uses
nrow <- 20
seed <- set.seed(123)
```

Our target variable will be a numeric - total abundance of the species in the area (in 1000's).

```{r}
set.seed(123)
abundance <- sample(10.5:60.5, nrow, rep = TRUE)
```

Proportion of plant parts used for the medicinal use (in percent).

```{r}
set.seed(123)
proportion_used <- sample(0.1:100, nrow, rep = TRUE)
```

Weighted distances to common harvesting sites (in km).

```{r}
set.seed(123)
distance <- sample(0.01:5.5, 500, rep = TRUE)
```

Use variable (i.e. the UR where it is = 1 if used for the medicinal use and 0 otherwise). See the `URs` function in [@R-ethnobotanyR]. 

```{r}
set.seed(123)
Use_medicine <- as.factor(sample(0:1, nrow, rep = TRUE))
```

Create the data frame called `eb_data` to hold all these simulated data.

```{r}
eb_data <- data.frame(abundance, proportion_used, distance, Use_medicine)
```

# Classical linear regression model

Now to highlight the difference between the Bayesian regression and linear regression. We will first use the `lm` function from the `stats` package [@R-base] to fit a linear model our simulated data.

```{r}
model_freq <- lm(abundance~., data = eb_data)
summary(model_freq)
```

Generally the model is evaluated using the `p.value` of each regressor and the Adjusted R-squared of the overall model. 

# Bayesian regression

We use the `stan_glm` from `rstanarm` [@R-rstanarm] to create a Bayesian generalized linear model via Stan [@R-rstan]. As with the linear regression above we use the `abundance~.` formula, i.e. `abundance` as the outcome variable of interest and all others as the regressors. The function will take our data and the priors to run a Markov chain Monte Carlo (MCMC). The prior distribution for the regression coefficients is kept in the default `prior = default_prior_coef(family)` for the `stan_glm` function in the `rstanarm` package [@R-rstanarm]. The default family for the prior is `gaussian`.

```{r ethno_bayes_model, results=FALSE}
ethno_bayes_model <- stan_glm(abundance~., data = eb_data, seed = seed)
```

Print the model with `print` function from base R [@R-base].

```{r print}
print(ethno_bayes_model, digits = 3)
```
 
The Median estimate is the median computed from the simulation, and `MAD_SD` is the median absolute deviation computed from the same simulation. We can plot the simulation of each predictor using the `mcmc_dens` function from `bayesplot` [@R-bayesplot]. The function plots histograms from Markov chain Monte Carlo (MCMC) models. We can use `vline_at`, which is one of the `bayesplot-helpers` from `bayesplot`, to create a line on the plot to show the median. 

```{r mcmc_dens-proportion_used}
line<-median(eb_data$proportion_used)
mcmc_dens(ethno_bayes_model, pars = c("proportion_used")) +
  vline_at(-0.095, col = "red")
```

```{r mcmc_dens-distance}
mcmc_dens(ethno_bayes_model, pars=c("distance"))+
  vline_at(-0.403, col="red")
```

```{r mcmc_dens-Use_medicine1}
mcmc_dens(ethno_bayes_model, pars=c("Use_medicine1")) +
  vline_at(9.497, col="red")
```

Now we evaluate the model parameters by analyzing the posteriors. We will look at the statistics shown in `describe_posterior` from `bayestestR` [@R-bayestestR].

```{r describe_posterior}
describe_posterior(posteriors = ethno_bayes_model)
```

The table shows (from left to right): 

- The `Median` of the distribution (as above). The credible Interval `95% CI`, which can be used to quantify the uncertainty about the regression coefficients (the default highest density interval `ci_method = "hdi"` shows the probability (given the data) that a coefficient lies above the low value and under high value). 
- The Probability of Direction `pd`, which is the probability that the effect goes to the positive or to the negative direction (a possible equivalent for the p-value). 
- The Region of Practical Equivalence `ROPE` is a defined range around zero which can be considered practically the same as no effect (zero). It is similar to the probability of getting an effect equal to zero (the null hypothesis in linear regression) as a range rather than a point. 
- The scale reduction factor `Rhat` computed for each scalar quantity of interest, as the standard deviation of that quantity from all the chains included together, divided by the root mean square of the separate within-chain standard deviations. When this value is close to 1 we do not have any convergence problem with MCMC. 
- The effective sample size `ESS`, which captures how many independent draws contain the same amount of information as the dependent sample obtained by the MCMC algorithm, the higher the ESS the better. The threshold used in practice is 400.

Alternatively, we can get the coefficient estimates (which are the medians by default) separately by using the `get_parameters` function from the `insight` library [@R-insight].

```{r posterior_samples}
posterior_samples <- get_parameters(ethno_bayes_model)
```

Use the `print` from base R and the `map_dbl` function from the `purrr` library [@R-purrr] to show the results for the `posterior_samples` (the coefficients of the model).

```{r print-posterior_samples}
print(purrr::map_dbl(posterior_samples, median), digits = 3)
```

As we see the values are closer to each other due to the like normality of the distribution of the posteriors where all the central statistics (mean, median, mode) are closer to each other. Using the following plot to visualize the proportion_used coefficient using different statistics as follows

```{r mcmc_dens-all_vline_at}
mcmc_dens(ethno_bayes_model, pars = c("proportion_used"))+
  vline_at(median(posterior_samples$proportion_used), col = "red")+
  vline_at(mean(posterior_samples$proportion_used), col = "yellow")+
  vline_at(map_estimate(posterior_samples$proportion_used), col="green")
```

As expected they are approximately on top of each other.

# Bayesian inferences

`hdi`: As we an alternative to the significance testing in classical regression (frequentist), we can test if the corresponding credible interval contains zero or not, if no then this coefficient can be considered important. Let's go back to our model and check the HDI of each coefficient.

```{r hdi}
hdi(ethno_bayes_model)
```

We can also check for the portion of the credible interval that falls inside the ROPE interval by calling the rope from `bayestestR` library.

```{r rope-proportion_used}
rope(posterior_samples$proportion_used)
```

Some of the credible interval of `proportion_used` variable is inside the ROPE interval. In other words, the probability of this coefficient to be zero is 60.72%.

```{r rope-Use_medicine1}
rope(posterior_samples$Use_medicine1)
```
 
For `Use_medicine1` all the credible interval (HDI) is outside the ROPE range, which corresponds to the linear regression finding that the coefficient is highly significant.
 
```{r rope-Intercept}
 rope(posterior_samples$`(Intercept)`)
```

The same thing is true for the `intercept` variable.

```{r rope-distance}
rope(posterior_samples$distance)
```

Some of the credible interval of distance variable is inside the ROPE interval. In other words, the probability of this coefficient to be zero is 12.79%.

```{r rope_range}
rope_range(ethno_bayes_model)
```

# The Probability of Direction and p-value

Sometimes we are only interested in the direction of the coefficient (positive or negative). This is the role of the `pd` statistic in the above table. A high `pd` value means that the associated effect is concentrated on the same side as the median. This probability is closely related to the `p-value` (p-value = 1 âˆ’ pd). 

```{r p-value-pd}
df1 <-dplyr::select(tidy(model_freq), c(term, p.value))
df1$p.value <- round(df1$p.value, digits = 3)
df2 <- 1- purrr::map_dbl(posterior_samples, p_direction)
df <- cbind(df1, df2)
```

This document was generated with knitr [@R-knitr] and rmarkdown [@R-rmarkdown].

# References
 
